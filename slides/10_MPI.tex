\documentclass{beamer}
\usetheme{metropolis}

\input{common.tex}

\title{Tutorium 10: Parallelprogrammierung mit MPI}
% \subtitle{}
\author{Paul Brinkmeier}
\institute{Tutorium Programmierparadigmen am KIT}
\date{18. Januar 2022}

\begin{document}

\begin{frame}
	\titlepage
\end{frame}

% {
%	\usebackgroundtemplate{\includegraphics[width=\paperwidth]{EulenfestWerbung}}
%	\begin{frame}[plain]
%	\end{frame}
% }

\section{Heutiges Programm}

\begin{frame}{Parallelprogrammierung}
	ProPa-Stoff zu Parallelprogrammierung:

	\begin{itemize}
		\item Grundlegende Begriffe
		\item Message Passing, wurde in OS \emph{kurz} behandelt (\enquote{message queues})
		\item Shared Memory + Synchronisierung, wie in SWT1, OS, etc.
		\begin{itemize}
			\item In Java, mit ein paar Details zur JVM
		\end{itemize}
	\end{itemize}
\end{frame}

\section{Begriffe}

\begin{frame}{Flynns Taxonomie}
	\begin{itemize}
		\item SISD: Single Instruction, Single Data\\
			{\footnotesize Ein Datum wird von einer Ausführungsarbeit bearbeitet}
		\item SIMD: Single Instruction, Multiple Data\\
			{\footnotesize Eine Ausführungseinheit bearbeitet mehrere Daten gleichzeitig}
		\item MIMD: Multiple Instruction, Multiple Data\\
			{\footnotesize $\approx$ Mehrere Ausführungseinheiten arbeiten gleichzeitig}
		\item MISD: Multiple Instruction, Single Data\\
			{\footnotesize $\approx$ Mehrere Ausführungseinheiten arbeiten gleichzeitig an einem Datum}
	\end{itemize}

	\pause
	Beispiele?
\end{frame}

\begin{frame}{Daten- und Taskparallelismus}
	Parallele Probleme sind üblicherweise entweder
	
	\begin{itemize}
		\item \enquote{datenparallel}: Problem kann auf identische Ausführungseinheiten verteilt werden\\
			Beispiel: \texttt{map primeFactors [1432793, 651433, ...]}

		\item \enquote{taskparallel}: Problembestandteile sind nicht homogen\\
			Beispiel: Videospiel mit Render-, Netzwerk- und Logikprozessen
	\end{itemize}

	Datenparallele Probleme sind i.d.R. einfacher zu behandeln (auch: \enquote{embarrassingly parallel}).
	Bei manchen Problemen verschwimmt die Grenze auch (bspw. Webserver).
\end{frame}

\section{MPI-Basics}

\begin{frame}{MPI}
	MPI (\enquote{Message Passing Interface}) ist ein Standard für Parallelprogrammierung.
	Es existieren verschiedene Implementierungen für verschiedene Sprachen.
	Die VL verwendet \href{https://www.open-mpi.org/}{Open MPI}, eine Open-Source-Implementierung.

	\begin{itemize}
		\item MPI-\enquote{Prozesse} beziehen sich i.d.R. auf Prozessorkerne
		\item Message Passing statt Shared Memory:
		\begin{itemize}
			\item Daten werden explizit über \texttt{Send} und \texttt{Recv} geteilt
		\end{itemize}
		\item MPI-Prozesse werden in sog. \emph{Communicators} eingeteilt. Wir verwenden immer den Communicator, der alle Prozesse enthält (\texttt{MPI\_COMM\_WORLD})
	\end{itemize}
\end{frame}

\subsection{Installation}

\begin{frame}{Installation von MPI}
	MPI-Beispiele gehen von Linux-Systemen aus, verwendet unter Windows bitte WSL.

	\begin{itemize}
		\item \texttt{apt install openmpi-bin} (Ubuntu)
		\item \texttt{pacman -S openmpi} (Arch Linux)
		\item \texttt{dnf install openmpi} (Fedora)
                \item \texttt{brew install open-mpi} (macOS)
	\end{itemize}

	Verwendet \texttt{mpicc --version} zum Testen der Installation.
\end{frame}

\begin{frame}{MPI: Grundgerüst}
  \code{code/mpi.c}
\end{frame}

\begin{frame}{MPI: Grundgerüst}
  Grundlegende Begriffe:

  \begin{itemize}
    \item \emph{Communicator}: Gruppe von Prozessen
    \item \texttt{MPI\_COMM\_WORLD}: Communicator, der alle Prozesse enthält
    \item \texttt{size}: Gesamtzahl der Prozesse
    \item \texttt{rank}: Laufende Nummer eines Prozesses $\in [0, \text{\texttt{size}})$
    \item \texttt{root}: Ausgangspunkt einer kollektiven Operation
  \end{itemize}
\end{frame}

\begin{frame}{Bauen und Ausführen von MPI-Programmen}
        MPI-Programme werden mit \texttt{mpicc} (Wrapper um \texttt{gcc} oder \texttt{clang}) kompiliert:

        \code{code/mpicompile.sh}

	Um ein Programm auszuführen, wird \texttt{mpirun} verwendet:

        \code{code/mpirun.sh}

	\begin{itemize}
		\item \texttt{N} ist die Zahl der Prozesse, die ausgeführt werden sollen
                \item Betrachtet die Demos \texttt{mpi/hello} und \texttt{mpi/sendrecv}.
	\end{itemize}
\end{frame}

\begin{frame}{Beispielausführung von \texttt{mpi/hello}}
  \begin{figure}
    \input{../demos/mpi/hello/perf.pgf}
  \end{figure}

  \begin{itemize}
    \item \emph{real}: Tatsächlich vergangene Zeit
    \item \emph{user}/\emph{sys}: Auf Prozessoren vergangene Zeit {\tiny (Im User- bzw. Kernelmode)}
	\pause
    \item Prozessor: 4 $\times$ Intel Core i5-7600K @ 3,8GHz
  \end{itemize}
\end{frame}

\subsection{Primitive}

\newcolumntype{C}{p{0.4cm}}

\begin{frame}{Send/Recv}
	Mit den Message-Passing-Primitiven \texttt{Send} und \texttt{Recv} werden Daten zwischen Prozessen ausgetauscht.\\
	Die Aufrufe sind unabhängig vom Medium (IPC, Sockets, ...).

	\begin{figure}
	\begin{tikzpicture}
		\node (lhs) {\begin{tabular}{|C|C|C|}
			\hline
			$A_0$ & & \\
			\hline
			& & \\
			\hline
			& & \\
			\hline
		\end{tabular}};
		\node (lhsLabelP) [left=0 of lhs] {\rotatebox{90}{\tiny Prozesse}};
		\node (lhsLabelD) [above=0 of lhs] {\tiny Daten};

		\node (rhs) [right=4cm of lhs] {\begin{tabular}{|C|C|C|}
			\hline
			$A_0$ & & \\
			\hline
			\textcolor{blue}{$A_0$} & & \\
			\hline
			& & \\
			\hline
		\end{tabular}};

		\draw[->, thick] (lhs) -- node[above] {$P_0$: \texttt{Send(dest=1)}} (rhs);
		\draw[->, thick] (lhs) -- node[below] {$P_1$: \texttt{Recv(source=0)}} (rhs);
	\end{tikzpicture}
	\end{figure}

	\begin{itemize}
		\item {\footnotesize \texttt{int MPI\_Send(buf, count, datatype, dest, tag, comm)}}
		\item {\footnotesize \texttt{int MPI\_Recv(buf, count, datatype, source, tag, comm, status)}}
	\end{itemize}
\end{frame}

\section{Kollektive Operationen}

\begin{frame}{Bcast}
	\texttt{Bcast} verteilt ein Datum auf alle Prozesse.

	\begin{figure}
	\begin{tikzpicture}
		\node (lhs) {\begin{tabular}{|C|C|C|}
			\hline
			$A_0$ & & \\
			\hline
			& & \\
			\hline
			& & \\
			\hline
		\end{tabular}};
		\node (lhsLabelP) [left=0 of lhs] {\rotatebox{90}{\tiny Prozesse}};
		\node (lhsLabelD) [above=0 of lhs] {\tiny Daten};

		\node (rhs) [right=4cm of lhs] {\begin{tabular}{|C|C|C|}
			\hline
			$A_0$ & & \\
			\hline
			\textcolor{blue}{$A_0$} & & \\
			\hline
			\textcolor{blue}{$A_0$} & & \\
			\hline
		\end{tabular}};

		\draw[->, thick] (lhs) -- node[above] {\texttt{Bcast(root=0)}} (rhs);
	\end{tikzpicture}
	\end{figure}

	\begin{itemize}
		\item {\footnotesize \texttt{int MPI\_Bcast(buf, count, datatype, root, comm)}}
		\item Daten befinden sich ursprünglich auf \texttt{root} 
		\begin{itemize}
			\item $\leadsto$ Fallunterscheidung in \texttt{Bcast}:
			\item {\footnotesize \texttt{if rank == root then forall others: send() else recv()}}
		\end{itemize}
	\end{itemize}

	\pause
	Implementiert \texttt{custom\_Bcast} in \texttt{demos/mpi/custom\_broadcast}!
\end{frame}

\begin{frame}{Scatter}
	\texttt{Scatter} verteilt eine Liste von Daten auf mehrere Prozesse.

	\begin{figure}
	\begin{tikzpicture}
		\node (lhs) {\begin{tabular}{|C|C|C|}
			\hline
			$A_0$ & $A_1$ & $A_2$ \\
			\hline
			& & \\
			\hline
			& & \\
			\hline
		\end{tabular}};
		\node (lhsLabelP) [left=0 of lhs] {\rotatebox{90}{\tiny Prozesse}};
		\node (lhsLabelD) [above=0 of lhs] {\tiny Daten};

		\node (rhs) [right=4cm of lhs] {\begin{tabular}{|C|C|C|}
			\hline
			\textcolor{blue}{$A_0$} & & \\
			\hline
			\textcolor{blue}{$A_1$} & & \\
			\hline
			\textcolor{blue}{$A_2$} & & \\
			\hline
		\end{tabular}};

		\draw[->, thick] (lhs) -- node[above] {\texttt{Scatter(root=0)}} (rhs);
	\end{tikzpicture}
	\end{figure}

	\begin{itemize}
		\item {\footnotesize \texttt{int MPI\_Scatter(sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm)}}
		\item \texttt{sendcount}, \texttt{recvcount}: Zahl der Elemente, die an einen Prozess verteilt werden
		\item I.d.R.: \texttt{sendcount == recvcount}
	\end{itemize}
\end{frame}

\begin{frame}{Gather}
	\texttt{Gather} sammelt Daten von allen Prozessen in einer Liste.

	\begin{figure}
	\begin{tikzpicture}
		\node (lhs) {\begin{tabular}{|C|C|C|}
			\hline
			$A_0$ & & \\
			\hline
			$B_0$ & & \\
			\hline
			$C_0$ & & \\
			\hline
		\end{tabular}};
		\node (lhsLabelP) [left=0 of lhs] {\rotatebox{90}{\tiny Prozesse}};
		\node (lhsLabelD) [above=0 of lhs] {\tiny Daten};

		\node (rhs) [right=4cm of lhs] {\begin{tabular}{|C|C|C|}
			\hline
			\textcolor{blue}{$A_0$} & \textcolor{blue}{$B_0$} & \textcolor{blue}{$C_0$} \\
			\hline
			& & \\
			\hline
			& & \\
			\hline
		\end{tabular}};

		\draw[->, thick] (lhs) -- node[above] {\texttt{Gather(root=0)}} (rhs);
	\end{tikzpicture}
	\end{figure}

	\begin{itemize}
		\item {\footnotesize \texttt{int MPI\_Gather(sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm)}}
		\item \texttt{sendcount}, \texttt{recvcount}: Zahl der Elemente, die an einen Prozess verteilt werden
		\item I.d.R.: \texttt{sendcount == recvcount}
	\end{itemize}
\end{frame}

\begin{frame}{Scatter und Gather}
	\texttt{Scatter} und \texttt{Gather} sind mehr oder weniger \enquote{invers}:

	\code{code/scatter.c}

    Häufiges Muster: \texttt{Scatter}, Daten bearbeiten, \texttt{Gather}, Ergebnisse zusammenführen
\end{frame}

\begin{frame}{Aufgabe zu Scatter und Gather}
	Implementiert folgendes Programm mit MPI:

	\begin{itemize}
		\item $N$: Prozessoranzahl (\texttt{MPI\_Comm\_size}), $x = 1000$
		\item $P_0$ legt \texttt{long}-Liste mit Elementen $[1, 2, ..., N \cdot x]$ an
		\item $P_i$ summiert einen $x$-Ausschnitt der Liste mit $i \in [0;N)$
		\item $P_0$ summiert die einzelnen Summen
	\end{itemize}

	Verwendet dafür:

	\begin{itemize}
		\item \texttt{MPI\_Comm\_size}, \texttt{MPI\_Comm\_rank}
		\item \texttt{MPI\_Scatter}
		\item \texttt{MPI\_Gather}
	\end{itemize}

	Dokumentation für MPI-Funktionen bekommt ihr mit \texttt{man <f>}
\end{frame}


\begin{frame}{Allgather}
	\texttt{Allgather} ist die \enquote{Verkettung} von \texttt{Gather} und \texttt{Bcast}.

	\begin{figure}
	\begin{tikzpicture}
		\node (lhs) {\begin{tabular}{|C|C|C|}
			\hline
			$A_0$ & & \\
			\hline
			$B_0$ & & \\
			\hline
			$C_0$ & & \\
			\hline
		\end{tabular}};
		\node (lhsLabelP) [left=0 of lhs] {\rotatebox{90}{\tiny Prozesse}};
		\node (lhsLabelD) [above=0 of lhs] {\tiny Daten};

		\node (rhs) [right=4cm of lhs] {\begin{tabular}{|C|C|C|}
			\hline
			\textcolor{blue}{$A_0$} & \textcolor{blue}{$B_0$} & \textcolor{blue}{$C_0$} \\
			\hline
			\textcolor{blue}{$A_0$} & \textcolor{blue}{$B_0$} & \textcolor{blue}{$C_0$} \\
			\hline
			\textcolor{blue}{$A_0$} & \textcolor{blue}{$B_0$} & \textcolor{blue}{$C_0$} \\
			\hline
		\end{tabular}};

		\draw[->, thick] (lhs) -- node[above] {\texttt{Allgather()}} (rhs);
	\end{tikzpicture}
	\end{figure}

	\begin{itemize}
		\item {\footnotesize \texttt{int MPI\_Allgather(sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm)}}
		\item Im Gegensatz zu \texttt{Gather} gibt es keinen Parameter \texttt{root}
	\end{itemize}
\end{frame}

\begin{frame}{Alltoall}
	\texttt{Alltoall} stückelt Daten von jedem Prozess und verteilt sie.

	\begin{figure}
	\begin{tikzpicture}
		\node (lhs) {\begin{tabular}{|C|C|C|}
			\hline
			$A_0$ & $A_1$ & $A_2$ \\
			\hline
			$B_0$ & $B_1$ & $B_2$ \\
			\hline
			$C_0$ & $C_1$ & $C_2$ \\
			\hline
		\end{tabular}};
		\node (lhsLabelP) [left=0 of lhs] {\rotatebox{90}{\tiny Prozesse}};
		\node (lhsLabelD) [above=0 of lhs] {\tiny Daten};

		\node (rhs) [right=4cm of lhs] {\begin{tabular}{|C|C|C|}
			\hline
			\textcolor{blue}{$A_0$} & \textcolor{blue}{$B_0$} & \textcolor{blue}{$C_0$} \\
			\hline
			\textcolor{blue}{$A_1$} & \textcolor{blue}{$B_1$} & \textcolor{blue}{$C_1$} \\
			\hline
			\textcolor{blue}{$A_2$} & \textcolor{blue}{$B_2$} & \textcolor{blue}{$C_2$} \\
			\hline
		\end{tabular}};

		\draw[->, thick] (lhs) -- node[above] {\texttt{Alltoall()}} (rhs);
	\end{tikzpicture}
	\end{figure}

	\begin{itemize}
		\item {\footnotesize \texttt{int MPI\_Alltoall(sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm)}}
		\item Es führt sozusagen jeder Prozess einmal \texttt{Scatter} aus
	\end{itemize}
\end{frame}

\begin{frame}{Reduce}
	\texttt{Reduce} wendet eine assoziative Operation auf verteilte Daten an.

	\begin{figure}
	\begin{tikzpicture}
		\node (lhs) {\begin{tabular}{|C|C|C|}
			\hline
			$A_0$ & & \\
			\hline
			$B_0$ & & \\
			\hline
			$C_0$ & & \\
			\hline
		\end{tabular}};
		\node (lhsLabelP) [left=0 of lhs] {\rotatebox{90}{\tiny Prozesse}};
		\node (lhsLabelD) [above=0 of lhs] {\tiny Daten};

		\node (rhs) [right=4cm of lhs] {\begin{tabular}{|p{2cm}|C|C|}
			\hline
			\textcolor{blue}{$A_0+B_0+C_0$} & & \\
			\hline
			& & \\
			\hline
			& & \\
			\hline
		\end{tabular}};

		\draw[->, thick] (lhs) -- node[above] {\texttt{Reduce(root=0,op=+)}} (rhs);
	\end{tikzpicture}
	\end{figure}

	\begin{itemize}
		\item {\footnotesize \texttt{int MPI\_Reduce(sendbuf, recvbuf, count, type, op, root, comm)}}
                \begin{itemize}
                  \item Beispiele für \texttt{op}: \texttt{MPI\_SUM}, \texttt{MPI\_PROD}, \texttt{MPI\_MIN}, \texttt{MPI\_MAX}, etc.
                \end{itemize}
		\item \textcolor{red}{\emph{Ungefähr}} dasselbe wie ein Fold!
		\pause
		\item Ersetzt den letzten Teil der Summenaufgabe durch einen Aufruf zu \texttt{Reduce}!
	\end{itemize}
\end{frame}

\begin{frame}{Allreduce}
	\texttt{Allreduce} ist die Verkettung von \texttt{Reduce} und \texttt{Bcast}.

	\begin{figure}
	\begin{tikzpicture}
		\node (lhs) {\begin{tabular}{|C|C|C|}
			\hline
			$A_0$ & & \\
			\hline
			$B_0$ & & \\
			\hline
			$C_0$ & & \\
			\hline
		\end{tabular}};
		\node (lhsLabelP) [left=0 of lhs] {\rotatebox{90}{\tiny Prozesse}};
		\node (lhsLabelD) [above=0 of lhs] {\tiny Daten};

		\node (rhs) [right=4cm of lhs] {\begin{tabular}{|p{2cm}|C|C|}
			\hline
			\textcolor{blue}{$A_0+B_0+C_0$} & & \\
			\hline
			\textcolor{blue}{$A_0+B_0+C_0$} & & \\
			\hline
			\textcolor{blue}{$A_0+B_0+C_0$} & & \\
			\hline
		\end{tabular}};

		\draw[->, thick] (lhs) -- node[above] {\footnotesize{\texttt{Allreduce(root=0,op=+)}}} (rhs);
	\end{tikzpicture}
	\end{figure}

	\begin{itemize}
		\item {\footnotesize \texttt{int MPI\_Allreduce(sendbuf, recvbuf, count, type, comm, op, comm)}}
                \item Wie bei \texttt{Allgather}/\texttt{Alltoall}: Kein \texttt{root}-Parameter
                \item \texttt{Reduce} und \texttt{Allreduce} funktionieren außerdem auch \enquote{mehrspaltig}, d.h. auch $A_1 + B_1 + C_1$ etc.
	\end{itemize}
\end{frame}

\section{Ende}

\begin{frame}{Ende}
  % TODO
	\begin{itemize}
		\item Im Campus-System könnt ihr euch bis zum 22.03. für die PP-Klausur anmelden
                \begin{itemize}
                  \item Termin: 08.04.2020 von 12:00 bis 14:00
                \end{itemize}
		\item Bis zum 15.02. könnt ihr euch \href{https://campus.studium.kit.edu/renewal/payment.php}{Rückmelden}
		\item Schöne Woche!
	\end{itemize}
\end{frame}

\end{document}
